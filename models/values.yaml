catalog:
  qwen3-06b-gpu:
    enabled: true
    features: ["TextGeneration"]
    url: "ollama://qwen3:0.6b"
    engine: OLlama
    minReplicas: 1
    resourceProfile: amd-gpu-rx9070xt:1

  deepseek-r1-mi300x:
    enabled: false
    features: [TextGeneration]
    url: hf://deepseek-ai/DeepSeek-R1
    engine: VLLM
    env:
      HIP_FORCE_DEV_KERNARG: "1"
      NCCL_MIN_NCHANNELS: "112"
      TORCH_BLAS_PREFER_HIPBLASLT: "1"
      VLLM_USE_TRITON_FLASH_ATTN: "0"
      VLLM_FP8_PADDING: "0"
    args:
      - --trust-remote-code
      # Currently only context length =< 32k supported.
      # See: https://github.com/ROCm/vllm/issues/375
      - --max-model-len=32768
      - --max-num-batched-token=32768
      - --max-num-seqs=1024
      - --num-scheduler-steps=10
      - --tensor-parallel-size=8
      - --gpu-memory-utilization=0.90
      - --disable-log-requests
      - --enable-chunked-prefill=false
      - --max-seq-len-to-capture=16384
      - --kv-cache-dtype=fp8
    resourceProfile: amd-gpu-mi300x:8
    targetRequests: 1024